[{"authors":["admin"],"categories":null,"content":"I'm a 2nd year Master student in Speech Processing \u0026amp; Machine Learning Laboratory at NTU EECS, supervised by Prof. Hung-Yi Lee\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://sunprinces.github.io/portfolio/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/portfolio/authors/admin/","section":"authors","summary":"I'm a 2nd year Master student in Speech Processing \u0026amp; Machine Learning Laboratory at NTU EECS, supervised by Prof. Hung-Yi Lee","tags":null,"title":"Jui-Yang Hsu","type":"authors"},{"authors":["Jui-Yang Hsu","Yuan-Jui Chen","Hung-Yi Lee"],"categories":[],"content":"","date":1588558978,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588558978,"objectID":"4e9941bc617b4417e69ea1fc91d6211b","permalink":"https://sunprinces.github.io/portfolio/publication/metaasr/","publishdate":"2020-02-05T10:22:58+08:00","relpermalink":"/portfolio/publication/metaasr/","section":"publication","summary":"In this paper, we proposed to apply meta learning approach for low-resource automatic speech recognition (ASR). We formulated ASR for different languages as different tasks, and meta-learned the initialization parameters from many pretraining languages to achieve fast adaptation on unseen target language, via recently proposed model-agnostic meta learning algorithm (MAML). We evaluated the proposed approach using six languages as pretraining tasks and four languages as target tasks. Preliminary results showed that the proposed method, MetaASR, significantly outperforms the state-of-the-art multitask pretraining approach on all target languages with different combinations of pretraining languages. In addition, since MAML's model-agnostic property, this paper also opens new research direction of applying meta learning to more speech-related applications.","tags":["ASR","Meta Learning"],"title":"Meta Learning for End-to-End Low-Resource Speech Recognition","type":"publication"},{"authors":["Chia-Hsuan Lee","Hung-Yi Lee","Szu-Lin Wu","Chi-Liang Liu","Wei Fang","Jui-Yang Hsu","Bo-Hsiang Tseng"],"categories":[],"content":"","date":1555132501,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555132501,"objectID":"99f575e224189090e089d8af357fdab8","permalink":"https://sunprinces.github.io/portfolio/publication/qa-journal/","publishdate":"2020-02-05T13:15:01+08:00","relpermalink":"/portfolio/publication/qa-journal/","section":"publication","summary":"Although multimedia or spoken content presents more attractive information than plain text content, the former is more difficult to display on a screen and be selected by a user. As a result, for humans, accessing large collections of spoken content is much more difficult and time-consuming than doing so for text content. It would therefore be helpful to develop machines which understand spoken content. In this paper, we propose two new tasks for machine comprehension of spoken content. The first is a listening comprehension test for TOEFL, a challenging academic English examination for English learners whose native languages are not English. We show that the proposed model outperforms the naive approaches and other neural network based models by exploiting the hierarchical structures of natural languages and the selective power of attention mechanism. For the second listening comprehension task – spoken SQuAD – we find that speech recognition errors severely impair machine comprehension; we propose the use of subword units to mitigate the impact of these errors.","tags":["QA"],"title":"Machine Comprehension of Spoken Content: TOEFL Listening Test and Spoken SQuAD","type":"publication"},{"authors":["Wei Fang","Jui-Yang Hsu","Hung-Yi Lee","Lin-Shan Lee"],"categories":[],"content":"","date":1480827805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1480827805,"objectID":"98b16e1cebc40edb5a46b241c1f36a46","permalink":"https://sunprinces.github.io/portfolio/publication/ham-qa/","publishdate":"2020-02-05T13:03:25+08:00","relpermalink":"/portfolio/publication/ham-qa/","section":"publication","summary":"Multimedia or spoken content presents more attractive information than plain text content, but the former is more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It’s therefore highly attractive to develop machines which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, a new task of machine comprehension of spoken content was proposed recently. The initial goal was defined as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native languages are not English. An Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture was also proposed for this task, which considered only the sequential relationship within the speech utterances. In this paper, we propose a new Hierarchical Attention Model (HAM), which constructs multi-hopped attention mechanism over tree-structured rather than sequential representations for the utterances. Improved comprehension performance robust with respect to ASR errors were obtained.","tags":["QA"],"title":"Hierarchical Attention Model for Improved Machine Comprehension of Spoken Content","type":"publication"}]